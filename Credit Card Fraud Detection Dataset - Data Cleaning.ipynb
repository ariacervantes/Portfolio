{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfcff68d",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection Dataset - Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38b36e",
   "metadata": {},
   "source": [
    "We start by importing the necessary Python libraries, in this case Pandas and Numpy, we'll be using to clean and process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6a2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275b4c6",
   "metadata": {},
   "source": [
    "We use Pandas to load the Credit Card Fraud Detection Dataset into a Pandas dataframe. This allows us to easily manipulate and clean the data using the tools provided by Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89db9c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1e6d7",
   "metadata": {},
   "source": [
    "Check for missing values: Missing values can be a common issue in datasets, and can cause problems for our analysis. We check for missing values in the dataset using the Pandas isnull() method, and find that there are none in this particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5555fc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before cleaning: \n",
      "Time      0\n",
      "V1        0\n",
      "V2        0\n",
      "V3        0\n",
      "V4        0\n",
      "V5        0\n",
      "V6        0\n",
      "V7        0\n",
      "V8        0\n",
      "V9        0\n",
      "V10       0\n",
      "V11       0\n",
      "V12       0\n",
      "V13       0\n",
      "V14       0\n",
      "V15       0\n",
      "V16       0\n",
      "V17       0\n",
      "V18       0\n",
      "V19       0\n",
      "V20       0\n",
      "V21       0\n",
      "V22       0\n",
      "V23       0\n",
      "V24       0\n",
      "V25       0\n",
      "V26       0\n",
      "V27       0\n",
      "V28       0\n",
      "Amount    0\n",
      "Class     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values before cleaning: \")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92275fa5",
   "metadata": {},
   "source": [
    "There's no duplicates but in case it is we have to remove duplicates and checking for missing values again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d04bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "add56dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after removing duplicates: \n",
      "Time      0\n",
      "V1        0\n",
      "V2        0\n",
      "V3        0\n",
      "V4        0\n",
      "V5        0\n",
      "V6        0\n",
      "V7        0\n",
      "V8        0\n",
      "V9        0\n",
      "V10       0\n",
      "V11       0\n",
      "V12       0\n",
      "V13       0\n",
      "V14       0\n",
      "V15       0\n",
      "V16       0\n",
      "V17       0\n",
      "V18       0\n",
      "V19       0\n",
      "V20       0\n",
      "V21       0\n",
      "V22       0\n",
      "V23       0\n",
      "V24       0\n",
      "V25       0\n",
      "V26       0\n",
      "V27       0\n",
      "V28       0\n",
      "Amount    0\n",
      "Class     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values again\n",
    "print(\"Missing values after removing duplicates: \")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01f937",
   "metadata": {},
   "source": [
    "The 'Time' column in the contains the number of seconds between each transaction and the first transaction in the dataset. By converting this column to datetime format, we can more easily manipulate and analyze the data based on time-related features, such as day of the week, hour of the day, etc. This can be especially useful if we want to explore patterns and trends in the data over time. Using 'to_datetime()' we convert the values in the 'Time' column to a datetime format that Pandas can recognize and work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec09662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Time' column to datetime format\n",
    "df['Time'] = pd.to_datetime(df['Time'], unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79e1c6",
   "metadata": {},
   "source": [
    "After converting the 'Time' column to datetime format, we print the data types of each column using the dtypes attribute of the Pandas dataframe. This allows us to check that the 'Time' column was successfully converted to datetime format, and also to check the data types of other columns in the dataset. This can be useful for identifying any other columns that need to be converted to a different data type for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92aca36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column: \n",
      "Time      datetime64[ns]\n",
      "V1               float64\n",
      "V2               float64\n",
      "V3               float64\n",
      "V4               float64\n",
      "V5               float64\n",
      "V6               float64\n",
      "V7               float64\n",
      "V8               float64\n",
      "V9               float64\n",
      "V10              float64\n",
      "V11              float64\n",
      "V12              float64\n",
      "V13              float64\n",
      "V14              float64\n",
      "V15              float64\n",
      "V16              float64\n",
      "V17              float64\n",
      "V18              float64\n",
      "V19              float64\n",
      "V20              float64\n",
      "V21              float64\n",
      "V22              float64\n",
      "V23              float64\n",
      "V24              float64\n",
      "V25              float64\n",
      "V26              float64\n",
      "V27              float64\n",
      "V28              float64\n",
      "Amount           float64\n",
      "Class              int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check the data types of each column\n",
    "print(\"Data types of each column: \")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0328be",
   "metadata": {},
   "source": [
    "Outliers are data points that are significantly different from the other data points in a dataset. In the case of the 'Amount' column, outliers could be transactions with unusually large or small amounts. These outliers could be due to errors in the data or they could be legitimate transactions that are simply outside the usual range.\n",
    "\n",
    "The code is calculating the first and third quartiles of the 'Amount' column and then calculating the interquartile range (IQR). The lower and upper bounds of the data are calculated as 1.5 times the IQR below the first quartile and above the third quartile, respectively. Any data points that fall outside these bounds are considered outliers and are printed out.\n",
    "\n",
    "Checking for outliers is an important step in data cleaning as they can affect the analysis or models trained on the data. Removing outliers can help to improve the accuracy of the analysis or models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22bdaf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Time        V1        V2        V3        V4        V5  \\\n",
      "2      1970-01-01 00:00:01 -1.358354 -1.340163  1.773209  0.379780 -0.503198   \n",
      "20     1970-01-01 00:00:16  0.694885 -1.361819  1.029221  0.834159 -1.191209   \n",
      "51     1970-01-01 00:00:36 -1.004929 -0.985978 -0.038039  3.710061 -6.631951   \n",
      "64     1970-01-01 00:00:42 -0.522666  1.009923  0.276470  1.475289 -0.707013   \n",
      "85     1970-01-01 00:00:55 -4.575093 -4.429184  3.402585  0.903915  3.002224   \n",
      "...                    ...       ...       ...       ...       ...       ...   \n",
      "284735 1970-01-02 23:58:47 -1.661169 -0.565425  0.294268 -1.549156 -2.301359   \n",
      "284748 1970-01-02 23:58:58  1.634178 -0.486939 -1.975967  0.495364  0.263635   \n",
      "284753 1970-01-02 23:59:03  1.465737 -0.618047 -2.851391  1.425282  0.893893   \n",
      "284757 1970-01-02 23:59:05 -1.757643 -0.982659  1.091540 -1.409539 -0.662159   \n",
      "284806 1970-01-02 23:59:52 -0.533413 -0.189733  0.703337 -0.506271 -0.012546   \n",
      "\n",
      "              V6        V7        V8        V9  ...       V21       V22  \\\n",
      "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
      "20      1.309109 -0.878586  0.445290 -0.446196  ... -0.295583 -0.571955   \n",
      "51      5.122103  4.371691 -2.006868 -0.278736  ...  1.393406 -0.381671   \n",
      "64      0.355243  1.559849 -0.399579 -0.479813  ...  0.172401  1.011543   \n",
      "85     -0.491078 -2.705393  0.666451  1.922216  ... -0.047365  0.853360   \n",
      "...          ...       ...       ...       ...  ...       ...       ...   \n",
      "284735  2.365956 -0.248881 -0.857361  0.137784  ...  1.432397 -0.257828   \n",
      "284748 -0.713049  0.459925 -0.336879  0.743676  ... -0.113197 -0.493594   \n",
      "284753 -0.958325  1.508074 -0.625691 -0.369824  ...  0.500426  0.870491   \n",
      "284757  0.046930  0.173241  0.364812  1.143381  ... -0.191471 -0.432979   \n",
      "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
      "\n",
      "             V23       V24       V25       V26       V27       V28   Amount  \\\n",
      "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   378.66   \n",
      "20     -0.050881 -0.304215  0.072001 -0.422234  0.086553  0.063499   231.71   \n",
      "51      0.969719  0.019445  0.570923  0.333278  0.857373 -0.075538  1402.95   \n",
      "64      0.069666  0.157820 -1.109224 -0.302369  0.318170  0.316910   243.66   \n",
      "85     -0.971600 -0.114862  0.408300 -0.304576  0.547785 -0.456297   200.01   \n",
      "...          ...       ...       ...       ...       ...       ...      ...   \n",
      "284735 -0.072471 -1.035804 -0.437889 -0.238543  0.365302 -0.448621   381.05   \n",
      "284748  0.001993  0.602533 -0.049936 -0.145522 -0.040554  0.024884   220.28   \n",
      "284753 -0.495410  0.676929  0.787688 -0.254918 -0.117200 -0.014737   337.54   \n",
      "284757 -0.292549 -0.281494 -0.484349  0.366751 -0.102628 -0.458915   200.00   \n",
      "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   217.00   \n",
      "\n",
      "        Class  \n",
      "2           0  \n",
      "20          0  \n",
      "51          0  \n",
      "64          0  \n",
      "85          0  \n",
      "...       ...  \n",
      "284735      0  \n",
      "284748      0  \n",
      "284753      0  \n",
      "284757      0  \n",
      "284806      0  \n",
      "\n",
      "[31685 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for outliers in the 'Amount' column\n",
    "q1 = df['Amount'].quantile(0.25)\n",
    "q3 = df['Amount'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - (1.5 * iqr)\n",
    "upper_bound = q3 + (1.5 * iqr)\n",
    "outliers = df[(df['Amount'] < lower_bound) | (df['Amount'] > upper_bound)]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9237d24",
   "metadata": {},
   "source": [
    "This is a good practice to follow after performing data cleaning, as it allows you to keep a separate record of the cleaned data and to easily access it for further analysis or modeling in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1caf490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new CSV file\n",
    "df.to_csv('cleaned_creditcard.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
